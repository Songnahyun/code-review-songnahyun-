import tensorflow as tf
import numpy as np
#tensorflow 라이브러리와 numpy라이브러리를 가져옵니다.

tf.enable_eager_execution()
#eager모드로 실행하기 위해서 eager_excution을 실행합니다.
tf.__version__

# Random
tf.set_random_seed(0) 
# for reproductubility random_seed를 초기화 시켜 다음에 실행할 때 동일하게 실행할 수 있도록 합니다.

# data and label
x1 = [ 73.,  93.,  89.,  96.,  73.] 
x2 = [ 80.,  88.,  91.,  98.,  66.]
x3 = [ 75.,  93.,  90., 100.,  70.]
Y  = [152., 185., 180., 196., 142.]
# x와y 데이터 값을 준비합니다.

# random weights
w1 = tf.Variable(tf.random_normal([1]))    
w2 = tf.Variable(tf.random_normal([1])) 
w3 = tf.Variable(tf.random_normal([1]))
b  = tf.Variable(tf.random_normal([1]))
# 초기값을 1로 지정 초기값은 아무 값이나 줘도 상관 없습니다.(보통 랜덤값), x데이터가 3개이므로 weight도 3개가 필요합니다.

learning_rate = 0.000001               
 # learning_rate는 작은 값으로 지정합니다.

for i in range(1000+1): 
# 업데이트를 1001번 합니다.
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:      
#GradientTape을 사용하고, 아래에 있는 변수들의 정보를 tape에 기록합니다.

        hypothesis = w1 * x1 +  w2 * x2 + w3 * x3 + b      
 # 가설값 
        cost = tf.reduce_mean(tf.square(hypothesis - Y))     
 # 가설에서 실제값을 뺸 오차 제곱의 평균 값으로 cost를 정의한 것

    # calculates the gradients of the cost
    w1_grad, w2_grad, w3_grad, b_grad = tape.gradient(cost, [w1, w2, w3, b]) 
    # 변수들의 대한 기울기 값을 구합니다. 기울기 값은 각각 w1_grad, w2_grad, w3_grad, b_grad에 할당됩니다.
.
    
    # update w1,w2,w3 and b
    w1.assign_sub(learning_rate * w1_grad) 
w2.assign_sub(learning_rate * w2_grad)
    w3.assign_sub(learning_rate * w3_grad)
    b.assign_sub(learning_rate * b_grad)
      # 4개의 gradient 값을 업데이트 하기 위해서 assign_sub를 사용합니다.
        업데이트 할 때 gradient값에 learning_rate를 곱한 후 그 값을 빼서 할당해줍니다.



    if i % 50 == 0:   
      print("{:5} | {:12.4f}".format(i, cost.numpy()))
      # 50번에 1번씩 출력합니다.
#    i       cost
    0 |   11325.9121
   50 |     135.3618           
  100 |      11.1817
  150 |       9.7940
  200 |       9.7687
  250 |       9.7587
  300 |       9.7489
  350 |       9.7389
  400 |       9.7292
  450 |       9.7194
  500 |       9.7096
  550 |       9.6999
  600 |       9.6903
  650 |       9.6806
  700 |       9.6709
  750 |       9.6612
  800 |       9.6517
  850 |       9.6421
  900 |       9.6325
  950 |       9.6229
 1000 |       9.6134
# 결과 값: i값은 0부터 100까지 증가하고 cost 큰 값에서 급격히 줄어든후 거의 일정한 결과 값을 확인해 볼 수 있습니다.

# Matrix
 data = np.array([
    # X1,   X2,    X3,   y
    [ 73.,  80.,  75., 152. ],
    [ 93.,  88.,  93., 185. ],
    [ 89.,  91.,  90., 180. ],
    [ 96.,  98., 100., 196. ],
    [ 73.,  66.,  70., 142. ]
], dtype=np.float32)
# 메트릭스로 표현하면 훨씬 더 간결하게 표현할 수 있습니다. 데이터를 열 별로 정리하여 한꺼번에 나타냅니다. 

# slice data
X = data[:, :-1]    
# numpy의 슬라이싱을 활용하여 데이터를 잘라낸 것이고, X는 열의 첫번째부터 마지막 열을 제외하고 5행 3열짜리 matrix가 됩니다.
y = data[:, [-1]]   
# Y는 행은 전체, 열은 마지막부분을 뜻합니다.

W = tf.Variable(tf.random_normal([3, 1])) 
# X값에 열이3개이므로 행이 3개가 필요하고 출력값은 하나이기 때문에 weight의 행도 하나가 됩니다.
b = tf.Variable(tf.random_normal([1]))    
# bias는 하나가 됩니다.

learning_rate = 0.000001           
# learning_rate값은 작은 상수 값으로 지정

# hypothesis, prediction function
def predict(X):         
#우리의 예측함수는 다음과 같이 정의할 수 있습니다.
    return tf.matmul(X, W) + b  
    # matmul를 하는데 x곱하기w에 b를 더합니다.(b는 생략할 수도 있음) 


n_epochs = 2000        
for i in range(n_epochs+1):
 # 총 2001회 epochs가 수행하도록 하는 것입니다.
    # tf.GradientTape() to record the gradient of the cost function
    with tf.GradientTape() as tape:
        cost = tf.reduce_mean((tf.square(predict(X) - y)))
    #epochs가 2000까지 진행이 되면서 cost가 어떻게 변해가는지 살펴봅니다.

    # calculates the gradients of the loss
    W_grad, b_grad = tape.gradient(cost, [W, b])
    #기울기 값을 구합니다.

    # updates parameters (W and b)
    W.assign_sub(learning_rate * W_grad
    b.assign_sub(learning_rate * b_grad)
    # W와 b 값을 지속적으로 업데이트 합니다.
    if i % 100 == 0:
        print("{:5} | {:10.4f}".format(i, cost.numpy()))
        #100번에 한 번씩 출력합니다.
        
epoch | cost
    0 |  5455.5903                         
  100 |    31.7443
  200 |    30.9326
  300 |    30.7894
  400 |    30.6468
  500 |    30.5055
  600 |    30.3644
  700 |    30.2242
  800 |    30.0849
  900 |    29.9463
 1000 |    29.8081
 1100 |    29.6710
 1200 |    29.5348
 1300 |    29.3989
 1400 |    29.2641
 1500 |    29.1299
 1600 |    28.9961
 1700 |    28.8634
 1800 |    28.7313
 1900 |    28.5997
 2000 |    28.4689
# 결과 값: epoch는 2000까지 진행이 되면서 cost는 처음에 큰 값이였는데 급속하게 줄어서 더 이상 줄어들지 않는 상태가 됩니다. (최적화가 빠르게 일어나게 됩니다.)

