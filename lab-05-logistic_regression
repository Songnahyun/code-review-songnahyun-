import tensorflow.contrib.eager as tfe       
# tensorflow 라이브러리를 가져옵니다.

tf.enable_eager_execution()                 
# eager 모드로 실행하기 위해서 eager_execution를 실행합니다.

dataset = tf.data.Dataset.form_tensor)_slices((x_train, y_train)).batch(len(x_train))   
# tf데이터를 통해서 원하는 x,y값을 실제 x의 길이만큼 batch로 학습 하겠다는 것을 dataset로 가져옵니다.
w = tf.Variable(tf.zeros([2,1]), name='weight')  
# 2행 1열로 모델을 만듭니다.
b = tf.Variable(tf.zeros([1]), name='bias')      
# bias 값을 만들어 원하는 값을 구합니다.

def logistic_regression(featrues):
  hypothesis = tf.div(1., 1, + tf.exp(tf.matmul(features, W) + b))
     # W(x) + b에 대한 linear한 값을 sigmoid funtion 시그모이드 함수를 구할 수 있습니다.
       logistic legression을 구하기 위한 hypothesis 를 그려낸다.
       
  return hypothesis
def loss_fn(features, labels):   
  hypothesis = logistic_regression(features)
  # features 값이 들어와 오류를 없애기 위해 hypothesis를 호출합니다.
  
  cost = -tf.reduce_mean(label * tf.log(loss_fn(hypothesis) + (1 - labels) * tf.log(1 - hypothesis))
return cost  
 # label 값과 hypothesis를 통해서 cost값을 구해낼 수 있게 됩니다.
 
def grad(hypothesis, features, labels): 
# 학습을 위해 hypothesis와 label을 구합니다.

    with tf.GradientTape() as tape:
        loss_value = loss_fn(hypothesis,labels)   
# loss를 통해 가설값과 실제값 비교한 loss_value 값을 구할 수 있습니다.

    return tape.gradient(loss_value, [W,b]) 
# gradient를 통해 실제 모델값을 계속 바꿔 나갈 수 있게 됩니다.
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)    
# GradientDescentOptimizer 를 통해 실제 이동할 learing_rate를 통한 값으로 optimizer값 선언 합니다.

for step in range(EPOCHS): 
# 함수를 EPOCHS 만큼 반복합니다.
    for features, labels in tfe.Iterator(dataset):  
    # dataset을 가져와서 그 데이터를 토대로 Iterator로 돌려서 실제 X값(features)과 Y값(labels)을 넣어가며 모델을 만듭니다.
        grads = grad(logistic_regression(features), features, labels) 
        # 위에서 나온 x값과 y값을 실제 가설에 대입하여 학습을 위한 grads를 만들 수 있습니다.
        optimizer.apply_gradients(grads_and_vars=zip(grads,[W,b])) 
        # w, b가 계속 업데이트 되며 cost 값을 이 줄어들면서 최적의 값을 나타낼 수 있습니다.
        
        if step % 100 == 0: 
           print("Iter: {}, Loss: {:.4f}".format(step, loss_fn(logistic_regression(features) ,labels)))
           # 100번 마다 Iter값과 Loss값이 줄어드는 값을 출력하기 위한 format(step, loss_fn(logistic_regression(features) ,labels))) 값을 출력합니다.



def accuracy_fn(hypothesis, labels):
 # 모델이 만들어 졌을 때 가설과 실제 값을 비교하기 위한 것입니다.
    predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32) 
    # hypohtesis값은 logistic funtion을 통해 나온 값이고 sigmoid funtion의 0과 1의 사이에 나온 것을 decision boundary(0.5)를 통해 예측된 값이 나오게 됩니다.
    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, labels), dtype=tf.int32))  
    # 실제 값과 예측 값을 비교하여 맞는지 아닌지를 accuracy로 출력하게 됩니다.
    return accuracy

test_acc = accuracy_fn(logistic_regression(x_test),y_test) 
# test_acc값을 x_test와 y_test를 넣어서 출력합니다. logistic_regression(x_test)부분이 hypothesis이고 이 값이 정확한지 출력합니다.

